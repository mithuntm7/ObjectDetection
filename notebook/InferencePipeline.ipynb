{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
     ]
    }
   ],
   "source": [
    "processor = OwlViTProcessor.from_pretrained(\"C:/Projects/ObjectDetection/transformers/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"C:/Projects/ObjectDetection/transformers/owlvit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"C:\\Projects\\ObjectDetection\\data\\order_images\\order_photo_1.jpeg\")\n",
    "texts = [[ \"food item\", \"a grocery item\", \"a packet\", \"fruits\"]]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "target_sizes = torch.Tensor([image.size[::-1]])\n",
    "# Convert outputs (bounding boxes and class logits) to COCO API\n",
    "results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n",
    "\n",
    "i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "text = texts[i]\n",
    "boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "score_threshold = 0.1\n",
    "thresholded_boxes = []\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    if score >= score_threshold:\n",
    "      thresholded_boxes.append(box)\n",
    "      #print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.Normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "img_tensor = transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mithu\\AppData\\Local\\Temp\\ipykernel_14772\\2206010338.py:3: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  box = torch.tensor(detect, dtype=int)\n"
     ]
    }
   ],
   "source": [
    "detected_objects  = []\n",
    "for detect in thresholded_boxes:\n",
    "    box = torch.tensor(detect, dtype=int)\n",
    "    object_tensor = (img_tensor[:, box[1]:box[3], box[0]:box[2]])/255\n",
    "    detected_objects.append(object_tensor)\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mithu\\anaconda3\\lib\\site-packages\\PIL\\Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "template = Image.open(\"..\\\\data\\\\template_images\\\\paneer.jpeg\").convert('RGB')\n",
    "template_tensor = transform(template)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0235), tensor(0.9765))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_tensor.min(), template_tensor.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mithu/.cache\\torch\\hub\\facebookresearch_barlowtwins_main\n",
      "c:\\Users\\mithu\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mithu\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "barlow_model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_representation = barlow_model(template_tensor.unsqueeze(0))\n",
    "object_representation = barlow_model(detected_objects[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3130, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(template_representation[0],object_representation[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6680bb41b3b21afd2fcaa2fd2d2f2c634d8159c3435ffc0aa2cb21155ca376d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
